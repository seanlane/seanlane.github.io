<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Sean Lane">
    <meta name="description" content="Sean Lane&#39;s personal website">
    <meta name="keywords" content="blog,developer,personal,software,engineer,utah">

    
    <title>
  PySpark and Latent Dirichlet Allocation · Sean Lane
</title>

    <link rel="canonical" href="https://sean.lane.sh/posts/2016/05/PySpark-and-Latent-Dirichlet-Allocation/">
    <link href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Source+Code+Pro:400,700" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css" integrity="sha256-oSrCnRYXvHG31SBifqP2PM1uje7SJUyX0nTwO2RJV54=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://unpkg.com/purecss@1.0.0/build/pure-min.css" integrity="sha384-nn4HPE8lTHyVtfCBi5yW9d20FjT8BJwUXyWZT9InLYax14RDjBj46LmSztkmNP9w" crossorigin="anonymous">
    <link rel="stylesheet" href="https://unpkg.com/purecss@1.0.0/build/grids-responsive-min.css">
  
    
    
    <link rel="stylesheet" href="https://sean.lane.sh/css/coder.min.f1211b4ed23b57b3f430d3bf2f297317076a8af695c889860f420b16dc57158d.css" integrity="sha256-8SEbTtI7V7P0MNO/LylzFwdqivaVyImGD0ILFtxXFY0=" crossorigin="anonymous" media="screen" />
  
  
  
  
    <link rel="stylesheet" href="https://sean.lane.sh/style.css">
  
    <link rel="icon" type="image/png" href="https://sean.lane.sh/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://sean.lane.sh/images/favicon-16x16.png" sizes="16x16">
  <meta name="generator" content="Hugo 0.121.1">
    <script src="//instant.page/1.2.1" type="module" integrity="sha384-/IkE5iZAM/RxPto8B0nvKlMzIyCWtYocF01PbGGp1qElJuxv9J4whdWBRtzZltWn"></script>
  </head>
  <body class=" ">
    <nav class="navigation pure-g">
  <section class="pure-u-1 pure-u-md-1-3">
    <div class="nav-left">
      
        <a class="nav-avatar" href="https://sean.lane.sh">
          <img src="https://sean.lane.sh/images/sean.png" alt="avatar" />
        </a>
      
      <a class="nav-title" href="https://sean.lane.sh">
        Sean Lane
      </a>
    </div>
  </section>
  <section class="pure-u-1 pure-u-md-1-3 nav-center">
    
      
        <a href="mailto:hi@sean.lane.sh"><i class="far fa-envelope fa-lg"></i></a>
      
        <a href="https://github.com/seanlane/"><i class="fab fa-github fa-lg"></i></a>
      
        <a href="https://www.linkedin.com/in/seantheolane"><i class="fab fa-linkedin fa-lg"></i></a>
      
        <a href="https://sean.lane.sh/posts/index.xml"><i class="fas fa-rss fa-lg"></i></a>
      
        <a href="https://twitter.com/seantheolane"><i class="fab fa-twitter fa-lg"></i></a>
      
    
  </section>
  <section class="pure-u-1 pure-u-md-1-3 nav-right">
    
      
        <a href="https://sean.lane.sh/">Home</a>
      
        <a href="https://sean.lane.sh/resume/">Resume</a>
      
        <a href="https://sean.lane.sh/posts/">Posts</a>
      
        <a href="https://sean.lane.sh/other/">Other</a>
      
    
  </section>
</nav>

    <main class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">PySpark and Latent Dirichlet Allocation</h1>
        </div>
        <div class="post-meta pure-g">
          <div class="posted-on pure-u-1 pure-u-sm-1-3">
            <i class="fas fa-calendar"></i>
            <time datetime='2016-05-10T00:00:00Z'>
              May 10, 2016
            </time>
          </div>
          <div class="reading-time pure-u-1 pure-u-sm-1-3 post-center">
            <i class="fas fa-clock"></i>
            Reading time: 11 minutes
          </div>
          <div class="pure-u-1 pure-u-sm-1-3 post-right">
            
            <div class="tags">
  <i class="fas fa-tag"></i>
    <a href="https://sean.lane.sh/tags/python/">Python</a></div>

          </div>
        </div>
      </header>
      <div>
        <p>This past semester, I had the chance to take two courses: Statistical Machine Learning from a Probabilistic Perspective (it&rsquo;s a bit of a mouthful) and Big Data Science &amp; Capstone. In the former, we had the chance to study the breadth of various statistical machine learning algorithms and processes that have flourished in recent years. This included a number of different topics ranging from Gaussian Mixture Models to Latent Dirichlet Allocation. In the latter, our class divided into groups to work on a capstone project with one of a number of great companies or organizations. It was only a 3 credit-hour course, so it was a less intensive project than a traditional capstone course that is a student&rsquo;s sole focus for an entire semester, but it was a great experience nonetheless. The Big Data science course taught us some fundamentals with big data science and normal data analysis (ETL, MapReduce, Hadoop, Weka, etc.) and then released us off into the wild blue yonder to see what we could accomplish with our various projects.</p>
<p>For the Big Data course, my team was actually assigned two projects:</p>
<ol>
<li>Attempting to track illness and outbreaks using social media</li>
<li>Creating a module for Apache PySpark to conduct Sensitivity Analysis of <code>pyspark.ml</code> models</li>
</ol>
<p>Both of these projects involved the use of Apache PySpark, and as a result I came to become familiar with it at a basic level. For a final project within the Statistical Machine Learning class, I considered how I could bring the experience of both together, and thought of using the LDA capabilities of PySpark in order to model some of the social media data that my Big Data group had already gathered. An idea of mine was that if we could cluster the social media content, then we could find further patterns or filter out bad data, for example. That said, when my class attempted to implement LDA models ourselves, it took a considerable amount of time to process, but I felt that using PySpark on a cluster of computers would allow me to utilize a respectable amount of the social media data we had gathered. I came across a few tutorials and examples of using LDA within Spark, but all of them that I found were written using Scala. It is not a very difficult leap from Spark to PySpark, but I felt that a version for PySpark would be useful to some.</p>
<h2 id="summary-explanation-of-latent-dirichlet-allocation">Summary explanation of Latent Dirichlet Allocation</h2>
<p>The article that I mostly referenced when completing my own analysis can be found here: <a href="https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html">Topic modeling with LDA: MLlib meets GraphX</a>. There, Joseph Bradley gives an apt description of what topic modeling is, how LDA covers it and what it could be used for. I&rsquo;ll attempt to briefly summarize his remarks and refer you to the Databrick&rsquo;s blog and other resources for deeper coverage. Topic modeling attempts to take &ldquo;documents&rdquo;, whether they are actual documents, sentences, tweets, etcetera, and infer the topic of the document. LDA attempts to do so by interpreting topics as unseen, or latent, distributions over all of the possible words (vocabulary) in all of the documents (corpus). This was originally developed for text analysis, but is being used in a number of different fields.</p>
<h2 id="example-in-pyspark">Example in PySpark</h2>
<p>This example will follow the LDA example given in the Databrick&rsquo;s blog post, but it should be fairly trivial to extend to whatever corpus that you may be working with. In this example, we will take articles from 3 newsgroups, process them using the LDA functionality of <code>pyspark.mllib</code> and see if we can validate the process by recognizing 3 distinct topics.</p>
<p>The step is to gather your corpus together. As I previously mentioned, we&rsquo;ll use the discussions from 3 newsgroups. The entire set can be found here: <a href="http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html">20 Newsgroups</a>. For this example, I picked 3 topics that seem to be fairly distinct from each other:</p>
<ul>
<li>comp.os.ms-windows.misc</li>
<li>rec.sport.baseball</li>
<li>talk.religion.misc</li>
</ul>
<p>I extracted the collection of discussions, and then put all of the discussions into one directory to form my corpus. Then we can point the PySpark script to this directory to pull the documents in. The entirety of the code used in this example can be found at the bottom of this post.</p>
<p>The first actual bit of code will initialize our SparkContext:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> defaultdict
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark <span style="color:#f92672">import</span> SparkContext
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.mllib.linalg <span style="color:#f92672">import</span> Vector, Vectors
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.mllib.clustering <span style="color:#f92672">import</span> LDA, LDAModel
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql <span style="color:#f92672">import</span> SQLContext
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> re
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>num_of_stop_words <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>      <span style="color:#75715e"># Number of most common words to remove, trying to eliminate stop words</span>
</span></span><span style="display:flex;"><span>num_topics <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>	            <span style="color:#75715e"># Number of topics we are looking for</span>
</span></span><span style="display:flex;"><span>num_words_per_topic <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>    <span style="color:#75715e"># Number of words to display for each topic</span>
</span></span><span style="display:flex;"><span>max_iterations <span style="color:#f92672">=</span> <span style="color:#ae81ff">35</span>         <span style="color:#75715e"># Max number of times to iterate before finishing</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize</span>
</span></span><span style="display:flex;"><span>sc <span style="color:#f92672">=</span> SparkContext(<span style="color:#e6db74">&#39;local&#39;</span>, <span style="color:#e6db74">&#39;PySPARK LDA Example&#39;</span>)
</span></span><span style="display:flex;"><span>sql_context <span style="color:#f92672">=</span> SQLContext(sc)
</span></span></code></pre></div><p>Then we&rsquo;ll pull in the data and tokenize it to form our global vocabulary:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>data <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>wholeTextFiles(<span style="color:#e6db74">&#39;newsgroup/files/*&#39;</span>)<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> data \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>map( <span style="color:#66d9ef">lambda</span> document: document<span style="color:#f92672">.</span>strip()<span style="color:#f92672">.</span>lower()) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>map( <span style="color:#66d9ef">lambda</span> document: re<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;[\s;,#]&#34;</span>, document)) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>map( <span style="color:#66d9ef">lambda</span> word: [x <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> word <span style="color:#66d9ef">if</span> x<span style="color:#f92672">.</span>isalpha()]) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>map( <span style="color:#66d9ef">lambda</span> word: [x <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> word <span style="color:#66d9ef">if</span> len(x) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">3</span>] )
</span></span></code></pre></div><p>Here we process the corpus by doing the following:</p>
<ol>
<li>Load each file as an individual document</li>
<li>Strip any leading or trailing whitespace</li>
<li>Convert all characters into lowercase where applicable</li>
<li>Split each document into words, separated by whitespace, semi-colons, commas, and octothorpes</li>
<li>Only keep the words that are all alphabetical characters</li>
<li>Only keep words larger than 3 characters</li>
</ol>
<p>This then leaves us with each document represented as a list of words that are hopefully more insightful than words like &ldquo;the&rdquo;, &ldquo;and&rdquo;, and other small words that we suspect are inconsequential to the topics we are hoping to find. The next step is to then generate our global vocabulary:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Get our vocabulary</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. Flat map the tokens -&gt; Put all the words in one giant list instead of a list per document</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. Map each word to a tuple containing the word, and the number 1, signifying a count of 1 for that word</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. Reduce the tuples by key, i.e.: Merge all the tuples together by the word, summing up the counts</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4. Reverse the tuple so that the count is first...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 5. ...which will allow us to sort by the word count</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>termCounts <span style="color:#f92672">=</span> tokens \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>flatMap(<span style="color:#66d9ef">lambda</span> document: document) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> word: (word, <span style="color:#ae81ff">1</span>)) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>reduceByKey( <span style="color:#66d9ef">lambda</span> x,y: x <span style="color:#f92672">+</span> y) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> tuple: (tuple[<span style="color:#ae81ff">1</span>], tuple[<span style="color:#ae81ff">0</span>])) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>sortByKey(<span style="color:#66d9ef">False</span>)
</span></span></code></pre></div><p>The above code performs the following steps:</p>
<ol>
<li>Flattens the corpus, aggregating all of the words into one giant list of words</li>
<li>Maps each word with the number <code>1</code>, indicate we count this word once</li>
<li>Reduce each word count, by finding all of the instances of any given word, and adding up their respective counts</li>
<li>Invert each tuple, so that the word count precedes each word&hellip;</li>
<li>&hellip;which then allows us to sort by the count for each word.</li>
</ol>
<p>We now have a sorted list of tuples, sorted in descending order of the number of time each word is in the corpus. We can then use this to remove the most common words, which will most likely be commons words (like &ldquo;the&rdquo;, &ldquo;and&rdquo;, &ldquo;from&rdquo;) that are most likely not distinctive to any given topic, and are equally likely to be found in all of the topics. We then identify which words to remove by setting deciding to remove <code>k</code> amount of words, find the count of word that is <code>k</code> deep in the list, and then removing any words with that amount or more of occurrences in the vocabulary. After this, we will then index each word, giving each word a unique id and then collect them into a map:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Identify a threshold to remove the top words, in an effort to remove stop words</span>
</span></span><span style="display:flex;"><span>threshold_value <span style="color:#f92672">=</span> termCounts<span style="color:#f92672">.</span>take(num_of_stop_words)[num_of_stop_words <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Only keep words with a count less than the threshold identified above, </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># and then index each one and collect them into a map</span>
</span></span><span style="display:flex;"><span>vocabulary <span style="color:#f92672">=</span> termCounts \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>filter(<span style="color:#66d9ef">lambda</span> x : x[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">&lt;</span> threshold_value) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">1</span>]) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>zipWithIndex() \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>collectAsMap()
</span></span></code></pre></div><p>This leaves us with a vocabulary that consists of tuples of words and their word counts, with the most common words removed. The next step is to represent each document as a vector of word counts. What this means is that instead of each document being formed of a sequence of words, we will have a list that is the size of the global vocabulary, and the value of each cell is the count of the word whose id is the index of that cell:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Convert the given document into a vector of word counts</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">document_vector</span>(document):
</span></span><span style="display:flex;"><span>    id <span style="color:#f92672">=</span> document[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>    counts <span style="color:#f92672">=</span> defaultdict(int)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> document[<span style="color:#ae81ff">0</span>]:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> token <span style="color:#f92672">in</span> vocabulary:
</span></span><span style="display:flex;"><span>            token_id <span style="color:#f92672">=</span> vocabulary[token]
</span></span><span style="display:flex;"><span>            counts[token_id] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    counts <span style="color:#f92672">=</span> sorted(counts<span style="color:#f92672">.</span>items())
</span></span><span style="display:flex;"><span>    keys <span style="color:#f92672">=</span> [x[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> counts]
</span></span><span style="display:flex;"><span>    values <span style="color:#f92672">=</span> [x[<span style="color:#ae81ff">1</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> counts]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> (id, Vectors<span style="color:#f92672">.</span>sparse(len(vocabulary), keys, values))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Process all of the documents into word vectors using the </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># `document_vector` function defined previously</span>
</span></span><span style="display:flex;"><span>documents <span style="color:#f92672">=</span> tokens<span style="color:#f92672">.</span>zipWithIndex()<span style="color:#f92672">.</span>map(document_vector)<span style="color:#f92672">.</span>map(list)
</span></span></code></pre></div><p>The final thing to do before actually beginning to run the model is to invert our vocabulary so that we can lookup each word based on it&rsquo;s id. This will allow us to see which words strongly correlate to which topics:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Get an inverted vocabulary, so we can look up the word by it&#39;s index value</span>
</span></span><span style="display:flex;"><span>inv_voc <span style="color:#f92672">=</span> {value: key <span style="color:#66d9ef">for</span> (key, value) <span style="color:#f92672">in</span> vocabulary<span style="color:#f92672">.</span>items()}
</span></span></code></pre></div><p>Now we open an output file, and train our model on the corpus with the desired amount of topics and maximum number of iterations:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Open an output file</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;output.txt&#34;</span>, <span style="color:#e6db74">&#39;w&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    lda_model <span style="color:#f92672">=</span> LDA<span style="color:#f92672">.</span>train(documents, k<span style="color:#f92672">=</span>num_topics, maxIterations<span style="color:#f92672">=</span>max_iterations)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    topic_indices <span style="color:#f92672">=</span> lda_model<span style="color:#f92672">.</span>describeTopics(maxTermsPerTopic<span style="color:#f92672">=</span>num_words_per_topic)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Print topics, showing the top-weighted 10 terms for each topic</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(topic_indices)):
</span></span><span style="display:flex;"><span>        f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;Topic #</span><span style="color:#e6db74">{0}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(len(topic_indices[i][<span style="color:#ae81ff">0</span>])):
</span></span><span style="display:flex;"><span>            f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{0}</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">{1}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(inv_voc[topic_indices[i][<span style="color:#ae81ff">0</span>][j]] \
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">.</span>encode(<span style="color:#e6db74">&#39;utf-8&#39;</span>), topic_indices[i][<span style="color:#ae81ff">1</span>][j]))
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{0}</span><span style="color:#e6db74"> topics distributed over </span><span style="color:#e6db74">{1}</span><span style="color:#e6db74"> documents and </span><span style="color:#e6db74">{2}</span><span style="color:#e6db74"> unique words</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span> \
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">.</span>format(num_topics, documents<span style="color:#f92672">.</span>count(), len(vocabulary)))
</span></span></code></pre></div><p>Obviously, you can take the output and do with it what you will, but here we will get an output file called <code>output.txt</code> which will list each of our three topics that we are hoping to see. You can play around with the <code>num_topics</code> to see how the model reacts, but since we know we have discussions that center around three distinct topics, we would have that having 3 topics would reflect that by clustering around words that align with each of those topics separately.</p>
<p>The continuation of this is to gather &ldquo;unlabeled&rdquo; data (as much as this can be called labeled), and to use LDA to perform topic modeling on your newly found corpus. I&rsquo;m still learning on how to go about that, but hopefully this has been of some help to anyone looking to get started with PySpark LDA.</p>
<hr>
<h2 id="appendix-heres-the-code">Appendix: Here&rsquo;s the code</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> defaultdict
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark <span style="color:#f92672">import</span> SparkContext
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.mllib.linalg <span style="color:#f92672">import</span> Vector, Vectors
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.mllib.clustering <span style="color:#f92672">import</span> LDA, LDAModel
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql <span style="color:#f92672">import</span> SQLContext
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> re
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>num_of_stop_words <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>      <span style="color:#75715e"># Number of most common words to remove, trying to eliminate stop words</span>
</span></span><span style="display:flex;"><span>num_topics <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>              <span style="color:#75715e"># Number of topics we are looking for</span>
</span></span><span style="display:flex;"><span>num_words_per_topic <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>    <span style="color:#75715e"># Number of words to display for each topic</span>
</span></span><span style="display:flex;"><span>max_iterations <span style="color:#f92672">=</span> <span style="color:#ae81ff">35</span>         <span style="color:#75715e"># Max number of times to iterate before finishing</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize</span>
</span></span><span style="display:flex;"><span>sc <span style="color:#f92672">=</span> SparkContext(<span style="color:#e6db74">&#39;local&#39;</span>, <span style="color:#e6db74">&#39;PySPARK LDA Example&#39;</span>)
</span></span><span style="display:flex;"><span>sql_context <span style="color:#f92672">=</span> SQLContext(sc)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Process the corpus:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. Load each file as an individual document</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. Strip any leading or trailing whitespace</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. Convert all characters into lowercase where applicable</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4. Split each document into words, separated by whitespace, semi-colons, commas, and octothorpes</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 5. Only keep the words that are all alphabetical characters</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 6. Only keep words larger than 3 characters</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>wholeTextFiles(<span style="color:#e6db74">&#39;newsgroup/files/*&#39;</span>)<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> data \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>map( <span style="color:#66d9ef">lambda</span> document: document<span style="color:#f92672">.</span>strip()<span style="color:#f92672">.</span>lower()) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>map( <span style="color:#66d9ef">lambda</span> document: re<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;[\s;,#]&#34;</span>, document)) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>map( <span style="color:#66d9ef">lambda</span> word: [x <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> word <span style="color:#66d9ef">if</span> x<span style="color:#f92672">.</span>isalpha()]) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>map( <span style="color:#66d9ef">lambda</span> word: [x <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> word <span style="color:#66d9ef">if</span> len(x) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">3</span>] )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Get our vocabulary</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. Flat map the tokens -&gt; Put all the words in one giant list instead of a list per document</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. Map each word to a tuple containing the word, and the number 1, signifying a count of 1 for that word</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. Reduce the tuples by key, i.e.: Merge all the tuples together by the word, summing up the counts</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4. Reverse the tuple so that the count is first...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 5. ...which will allow us to sort by the word count</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>termCounts <span style="color:#f92672">=</span> tokens \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>flatMap(<span style="color:#66d9ef">lambda</span> document: document) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> word: (word, <span style="color:#ae81ff">1</span>)) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>reduceByKey( <span style="color:#66d9ef">lambda</span> x,y: x <span style="color:#f92672">+</span> y) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> tuple: (tuple[<span style="color:#ae81ff">1</span>], tuple[<span style="color:#ae81ff">0</span>])) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>sortByKey(<span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Identify a threshold to remove the top words, in an effort to remove stop words</span>
</span></span><span style="display:flex;"><span>threshold_value <span style="color:#f92672">=</span> termCounts<span style="color:#f92672">.</span>take(num_of_stop_words)[num_of_stop_words <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Only keep words with a count less than the threshold identified above, </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># and then index each one and collect them into a map</span>
</span></span><span style="display:flex;"><span>vocabulary <span style="color:#f92672">=</span> termCounts \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>filter(<span style="color:#66d9ef">lambda</span> x : x[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">&lt;</span> threshold_value) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">1</span>]) \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>zipWithIndex() \
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>collectAsMap()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Convert the given document into a vector of word counts</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">document_vector</span>(document):
</span></span><span style="display:flex;"><span>    id <span style="color:#f92672">=</span> document[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>    counts <span style="color:#f92672">=</span> defaultdict(int)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> document[<span style="color:#ae81ff">0</span>]:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> token <span style="color:#f92672">in</span> vocabulary:
</span></span><span style="display:flex;"><span>            token_id <span style="color:#f92672">=</span> vocabulary[token]
</span></span><span style="display:flex;"><span>            counts[token_id] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    counts <span style="color:#f92672">=</span> sorted(counts<span style="color:#f92672">.</span>items())
</span></span><span style="display:flex;"><span>    keys <span style="color:#f92672">=</span> [x[<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> counts]
</span></span><span style="display:flex;"><span>    values <span style="color:#f92672">=</span> [x[<span style="color:#ae81ff">1</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> counts]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> (id, Vectors<span style="color:#f92672">.</span>sparse(len(vocabulary), keys, values))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Process all of the documents into word vectors using the </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># `document_vector` function defined previously</span>
</span></span><span style="display:flex;"><span>documents <span style="color:#f92672">=</span> tokens<span style="color:#f92672">.</span>zipWithIndex()<span style="color:#f92672">.</span>map(document_vector)<span style="color:#f92672">.</span>map(list)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Get an inverted vocabulary, so we can look up the word by it&#39;s index value</span>
</span></span><span style="display:flex;"><span>inv_voc <span style="color:#f92672">=</span> {value: key <span style="color:#66d9ef">for</span> (key, value) <span style="color:#f92672">in</span> vocabulary<span style="color:#f92672">.</span>items()}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Open an output file</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;output.txt&#34;</span>, <span style="color:#e6db74">&#39;w&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    lda_model <span style="color:#f92672">=</span> LDA<span style="color:#f92672">.</span>train(documents, k<span style="color:#f92672">=</span>num_topics, maxIterations<span style="color:#f92672">=</span>max_iterations)
</span></span><span style="display:flex;"><span>    topic_indices <span style="color:#f92672">=</span> lda_model<span style="color:#f92672">.</span>describeTopics(maxTermsPerTopic<span style="color:#f92672">=</span>num_words_per_topic)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Print topics, showing the top-weighted 10 terms for each topic</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(topic_indices)):
</span></span><span style="display:flex;"><span>        f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;Topic #</span><span style="color:#e6db74">{0}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(len(topic_indices[i][<span style="color:#ae81ff">0</span>])):
</span></span><span style="display:flex;"><span>            f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{0}</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">{1}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(inv_voc[topic_indices[i][<span style="color:#ae81ff">0</span>][j]] \
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">.</span>encode(<span style="color:#e6db74">&#39;utf-8&#39;</span>), topic_indices[i][<span style="color:#ae81ff">1</span>][j]))
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    f<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{0}</span><span style="color:#e6db74"> topics distributed over </span><span style="color:#e6db74">{1}</span><span style="color:#e6db74"> documents and </span><span style="color:#e6db74">{2}</span><span style="color:#e6db74"> unique words</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span> \
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">.</span>format(num_topics, documents<span style="color:#f92672">.</span>count(), len(vocabulary)))
</span></span></code></pre></div>
      </div>
      <div id="comments" class="comments">
    <h2>Comments</h2>
    <div id="comments-preamble">
        To leave a comment, <a href="https://github.com/seanlane/seanlane.github.io/issues/2">visit this post's issue page on GitHub.</a> 
        A GitHub account is required, since <a href="https://sean.lane.sh/posts/2016/01/Hosting-comments-within-issues-on-Github-Pages/">I host this site's comments on GitHub.</a>
    <div>
</div>
<script type="text/javascript" src="https://unpkg.com/jquery@3.3.1/dist/jquery.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/datejs/1.0/date.min.js"></script>
<script type="text/javascript">
  function loadComments(data) {
    for (var i=0; i < data.length; i++) {
      var cuser = data[i].user.login;
      var cuserlink = "https://www.github.com/" + data[i].user.login;
      var clink = "https://github.com/seanlane\/seanlane.github.io/issues/2#issuecomment-" + 
          data[i].url.substring(data[i].url.lastIndexOf("/") + 1);
      var cbody = data[i].body_html;
      var cavatarlink = data[i].user.avatar_url;
      var cdate = Date.parse(data[i].created_at).setTimezone('UTC').toString("yyyy-MM-dd HH:mm:ss");

      $("#comments").append("<div class='comment'><div class='commentheader'><div class='commentgravatar'>" 
        + '<img src="' + cavatarlink + '" alt="" width="20" height="20">' 
        + "</div><a class='commentuser' href=\"" + cuserlink + "\">" 
        + cuser + "</a><a class='commentdate' href=\"" + clink 
        + "\">" + cdate + "</a></div><div class='commentbody'>" + cbody + "</div></div>");
    }
  }

  $.ajax("https://api.github.com/repos/seanlane\/seanlane.github.io/issues/2/comments", {
    headers: {Accept: "application/vnd.github.full+json"},
    success: function(msg){
      loadComments(msg);
    }
  });
</script>
      <footer>
        
      </footer>
    </article>
    
  </section>

    </main>
    <footer class="footer">
  
   © 2024
  
     · 
    Powered by <a href="https://gohugo.io/">Hugo</a> &amp; Extended from <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>
  
  
</footer>

    
<script>
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-31520487-4', 'auto');
	
	ga('send', 'pageview');
}
</script>
  </body>
</html>